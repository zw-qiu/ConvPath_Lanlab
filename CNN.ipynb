{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从每个ROI文件夹随机提取训练集(0.7)验证集(0.1)和测试集(0.2)并整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## 从每个ROI文件夹随机提取训练集(0.7)验证集(0.1)和测试集(0.2)并整合\n",
    "#\n",
    "# +\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from shutil import copyfile\n",
    "from scipy import io\n",
    "from PIL import Image\n",
    "##读取mat\n",
    "def LoadInMat(path):\n",
    "        # the path should also including the name of the .mat file\n",
    "        File_Disp = io.loadmat(path)\n",
    "        # Establish an empty list to save the valid name of variables\n",
    "        NameOfVariable = []\n",
    "        for key in File_Disp.keys():\n",
    "            if key == '__globals__' or key == '__version__' or key == '__header__':\n",
    "                continue\n",
    "            else:\n",
    "                NameOfVariable.append(key)\n",
    "        MatData = {}\n",
    "        for t in range(len(NameOfVariable)):\n",
    "            MatData[NameOfVariable[t]] = File_Disp[NameOfVariable[t]].tolist()\n",
    "        MatData['Keys'] = NameOfVariable\n",
    "        return MatData\n",
    "\n",
    "path0 = os.getcwd() #获取当前代码文件的路径\n",
    "path1=path0+'\\ROIs'#进入存放ROIs的文件夹\n",
    "ROIdirs = glob.glob(path1+'\\*')#每个ROIs文件夹的路径\n",
    "numROIdirs=len(ROIdirs)#计算ROIs文件夹的个数#10\n",
    "\n",
    "for i in range(1,numROIdirs+1):\n",
    "    temp = glob.glob(path1+'\\ROI'+str(i)+'\\*')#每个ROIs的路径\n",
    "    numROIs = len(temp)#计算ROIs的个数(80*80 patch数)\n",
    "    arr = np.array(range(1,numROIs+1,1))#1--num(ROIs)\n",
    "    nindex = np.random.permutation(arr)#随机排列\n",
    "    ntrain = math.floor(numROIs*0.7) #70%训练集,向下取整\n",
    "    nvalid = math.floor(numROIs*0.1) #10%验证集\n",
    "    ntest = numROIs-ntrain-nvalid #20%测试集\n",
    "    trainind = nindex[0:ntrain]\n",
    "    validind = nindex[ntrain:(ntrain+nvalid)]\n",
    "    testind = nindex[(ntrain+nvalid):numROIs]\n",
    "    ##提取文件到指定文件夹\n",
    "    labelmat=LoadInMat(path0+'\\\\Labels\\\\382412-2_'+str(i)+'.mat')\n",
    "    #{'label': [[2, 2, 2, 2, 2, 3, 2, 2, 3,...,2]], 'Keys': ['label']}\n",
    "    label=labelmat['label'][0]#type:list\n",
    "    #[2, 2, 2, 2, 2, 3, 2, 2, 3,...,2]\n",
    "    labels=[]\n",
    "    for j in trainind:\n",
    "        img = np.asarray(Image.open(path1+'\\ROI'+str(i)+'\\ROI_382412-2_' + str(i) +'_Patch_'+str(j)+'.jpg'))\n",
    "        Output = {'img': img, 'label': label[j-1]}\n",
    "        io.savemat(path0+'\\\\train\\\\ROI_382412-2_' + str(i) +'_Patch_'+str(j)+'.mat', Output)\n",
    "    for j in validind:\n",
    "        img = np.asarray(Image.open(path1+'\\ROI'+str(i)+'\\ROI_382412-2_' + str(i) +'_Patch_'+str(j)+'.jpg'))\n",
    "        Output = {'img': img, 'label': label[j-1]}\n",
    "        io.savemat(path0+'\\\\valid\\\\ROI_382412-2_' + str(i) +'_Patch_'+str(j)+'.mat', Output)\n",
    "    for j in testind:\n",
    "        img = np.asarray(Image.open(path1+'\\ROI'+str(i)+'\\ROI_382412-2_' + str(i) +'_Patch_'+str(j)+'.jpg'))\n",
    "        Output = {'img': img, 'label': label[j-1]}\n",
    "        io.savemat(path0+'\\\\test\\\\ROI_382412-2_' + str(i) +'_Patch_'+str(j)+'.mat', Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据、变换及归一化,构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "#from torchvision import transforms as transforms\n",
    "\n",
    "def LoadInMat(path):\n",
    "        # the path should also including the name of the .mat file\n",
    "        File_Disp = io.loadmat(path)\n",
    "        # Establish an empty list to save the valid name of variables\n",
    "        NameOfVariable = []\n",
    "        for key in File_Disp.keys():\n",
    "            if key == '__globals__' or key == '__version__' or key == '__header__':\n",
    "                continue\n",
    "            else:\n",
    "                NameOfVariable.append(key)\n",
    "        MatData = {}\n",
    "        for t in range(len(NameOfVariable)):\n",
    "            MatData[NameOfVariable[t]] = File_Disp[NameOfVariable[t]].tolist()\n",
    "        MatData['Keys'] = NameOfVariable\n",
    "        return MatData\n",
    "\n",
    "def default_loader(path):\n",
    "    DictMat = LoadInMat(path)\n",
    "    img = DictMat['img']\n",
    "    label = DictMat['label'][0][0]-1\n",
    "    return img, label\n",
    "\n",
    "class customData(Dataset):\n",
    "    def __init__(self, PathofDataset='E:\\\\Jupyter-notebook\\\\ConvPath\\\\train\\\\*', train=' ',\n",
    "                 loader=default_loader,transform=''):\n",
    "        self.listofDataset = glob.glob(PathofDataset)\n",
    "        self.loader = loader\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.listofDataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.listofDataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img, label = self.loader(self.listofDataset[item])\n",
    "        img = np.asarray(img)\n",
    "        img = img.astype(np.uint8)\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        img = self.transform(img)\n",
    "# #     np.swapaxes(img,0,2)\n",
    "# #     np.swapaxes(img,0,1)\n",
    "        return img, label\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 3 input image channel, 20 output channels, 3*3 square convolution\n",
    "        self.conv1 = nn.Conv2d(3, 20, 3)\n",
    "        self.pool1 = nn.MaxPool2d(3, 3)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 3)\n",
    "        self.pool2 = nn.MaxPool2d(3, 3)\n",
    "        self.conv3 = nn.Conv2d(20, 20, 3)\n",
    "        self.pool3 = nn.MaxPool2d(3, 3)\n",
    "        self.fc1 = nn.Linear(20 * 2 * 2, 20)\n",
    "        self.fc2 = nn.Linear(20, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 20 * 2 * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #         x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "#旋转和翻转图像\n",
    "transform = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),  #先四周填充0，在把图像随机裁剪成32*32\n",
    "    transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n",
    "    transforms.RandomRotation((-90,90)),#随机旋转\n",
    "    transforms.ToTensor(),#torchvision的输出是[0,1]的PILImage图像\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.229, 0.224, 0.225)), #R,G,B每层的归一化用到的均值和方差\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (1., 1., 1.))#归一化到[-0.5,0.5]\n",
    "])\n",
    "\n",
    "## Load dataset\n",
    "#DataLoader为我们提供了对Dataset的读取操作，常用参数有：batch_size(每个batch的大小),\n",
    " #shuffle(是否进行shuffle操作), num_workers(加载数据的时候使用几个子进程)\n",
    "\n",
    "Train_datasets = customData(PathofDataset='C:\\\\Python\\\\JupyterNotebook\\\\ConPath\\\\train1\\\\*',\n",
    "                            train=True, loader=default_loader, transform=transform)\n",
    "Train_dataloaders = torch.utils.data.DataLoader(Train_datasets,batch_size=20,\n",
    "                                                shuffle=True, num_workers=4)\n",
    "\n",
    "Valid_datasets = customData(PathofDataset='C:\\\\Python\\\\JupyterNotebook\\\\ConPath\\\\valid1\\\\*',\n",
    "                           train=False, loader=default_loader, transform=transform)\n",
    "Valid_dataloaders = torch.utils.data.DataLoader(Valid_datasets,batch_size=20,\n",
    "                                               #shuffle=True,\n",
    "                                                num_workers=4)\n",
    "Test_datasets = customData(PathofDataset='C:\\\\Python\\\\JupyterNotebook\\\\ConPath\\\\test1\\\\*',\n",
    "                           train=False, loader=default_loader, transform=transform)\n",
    "Test_dataloaders = torch.utils.data.DataLoader(Test_datasets,batch_size=20,\n",
    "                                               #shuffle=True,\n",
    "                                                num_workers=4)\n",
    "\n",
    "#classes = ('Tumor cell', 'stroma cell', 'lymphocyte')\n",
    "#print(Train_dataloaders,Test_dataloaders)\n",
    "\n",
    "net = Net().cuda()\n",
    "#print(net)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9,weight_decay=0.0001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络，训练集和验证集准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_save=[]\n",
    "correct_train=[]\n",
    "total_train=[]\n",
    "correct_valid=[]\n",
    "total_valid=[]\n",
    "if __name__==\"__main__\":\n",
    "    for epoch in tqdm(range(20)):  # 多批次循环\n",
    "        class_correct_train = list(0. for i in range(3))\n",
    "        class_total_train = list(0. for i in range(3))\n",
    "        class_correct_valid = list(0. for i in range(3))\n",
    "        class_total_valid = list(0. for i in range(3))\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for stage in ['train','valid']:\n",
    "            if stage == 'train':\n",
    "                dataloader = Train_dataloaders\n",
    "                net.train(True)\n",
    "            else:\n",
    "                dataloader = Valid_dataloaders\n",
    "                net.train(False)\n",
    "\n",
    "            for data in dataloader:\n",
    "                count += 1\n",
    "                #i, data in enumerate(Train_dataloaders,0):\n",
    "                # 获取输入\n",
    "                inputs, labels = data\n",
    "                inputs=inputs.cuda()\n",
    "                labels = labels.long().cuda()\n",
    "                # 梯度置0\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 正向传播，反向传播，优化\n",
    "                # t1 = time.time()\n",
    "                outputs = net(inputs)\n",
    "                # print('time: ' + str(t2-t1))\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                # 打印状态信息\n",
    "                running_loss += loss.item()\n",
    "                if count % 100 == 99:    # 每100批次打印一次\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 100))\n",
    "                    loss_save.append(running_loss)\n",
    "                    #print('running_loss:'+str(loss_save))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "    #             print(outputs)\n",
    "    #             print(labels)\n",
    "    #             print(inputs)\n",
    "                c = (predicted == labels).squeeze()\n",
    "                if stage == 'train':\n",
    "                    for i in range(len(labels.cpu().numpy().tolist())):\n",
    "                    #for i in range(10):\n",
    "                        label = labels[i]\n",
    "                        class_correct_train[label] += c[i].item()\n",
    "                        class_total_train[label] += 1\n",
    "                        #print('class_correct_train: ' + str(class_correct_train) + 'class_total_train: ' + str(class_total_train))\n",
    "\n",
    "                if stage == 'valid':\n",
    "                    for i in range(len(labels.cpu().numpy().tolist())):\n",
    "                    #for i in range(10):\n",
    "                        label = labels[i]\n",
    "                        class_correct_valid[label] += c[i].item()\n",
    "                        class_total_valid[label] += 1\n",
    "                        #print('class_correct_valid: ' + str(class_correct_valid) + 'class_total_valid: ' + str(class_total_valid))\n",
    "        correct_train.append(class_correct_train)\n",
    "        correct_valid.append(class_correct_valid)\n",
    "        total_train.append(class_total_train)\n",
    "        total_valid.append(class_total_valid)\n",
    "        Output = {'loss_save': loss_save,'correct_train':correct_train,'total_train':total_train,'correct_valid':correct_valid,'total_valid':total_valid}\n",
    "        io.savemat(\"C:\\\\Python\\\\JupyterNotebook\\\\ConPath\\\\result\\\\loss.mat\", Output)\n",
    "        torch.save(net, \"C:\\\\Python\\\\JupyterNotebook\\\\ConPath\\\\result\\\\net_Trained.pth\")\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 测试集测试及准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T06:53:31.926750Z",
     "start_time": "2019-09-27T06:50:55.739741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Testing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from scipy import io\n",
    "\n",
    "net = Net()\n",
    "net_dict = net.state_dict()#网络参数的字典\n",
    "#print(net_dict)\n",
    "classifier = torch.load(\"C:\\\\Python\\\\JupyterNotebook\\\\ConPath\\\\result\\\\net_Trained.pth\")\n",
    "classifier_dict = classifier.state_dict()#提取字典：网络参数\n",
    "\n",
    "#trained_dict = {k: v for k, v in classifier_dict.items() if k in net_dict}#提取字典里的参数\n",
    "trained_dict = {k: v.cpu() for k, v in classifier_dict.items() if k in net_dict}#提取字典里的参数\n",
    "net_dict.update(trained_dict)\n",
    "net.load_state_dict(net_dict)\n",
    "\n",
    "#outputs = net(inputs)\n",
    "\n",
    "correct_test=[]\n",
    "total_test=[]\n",
    "class_correct_test = list(0. for i in range(3))\n",
    "class_total_test = list(0. for i in range(3))\n",
    "\n",
    "for data in Test_dataloaders:\n",
    "    # 获取输入\n",
    "    inputs, labels = data\n",
    "    labels = labels.long()\n",
    "    #输出\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "#             print(outputs)\n",
    "#             print(labels)\n",
    "#             print(inputs)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(len(labels.numpy().tolist())):\n",
    "    #for i in range(10):\n",
    "        label = labels[i]\n",
    "        class_correct_test[label] += c[i].item()\n",
    "        class_total_test[label] += 1\n",
    "        #print('class_correct_test: ' + str(class_correct_test) + 'class_total_test: ' + str(class_total_test))\n",
    "\n",
    "correct_test.append(class_correct_test)\n",
    "total_test.append(class_total_test)\n",
    "Output = {'correct_test':correct_test,'total_test':total_test}\n",
    "io.savemat(\"C:\\\\Python\\\\JupyterNotebook\\\\ConPath\\\\result\\\\test_predict.mat\", Output)\n",
    "print('Finished Testing')\n",
    "\n",
    "\n",
    "#for i in range(3):\n",
    "    #print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Pytorch for DeepLearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
